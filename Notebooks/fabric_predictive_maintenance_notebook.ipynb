{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries and Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Data From KQL DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install Kqlmagic --no-cache-dir  --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kusto_uri = \"https://trd-qtekrqc7c4upzq11vm.z9.kusto.fabric.microsoft.com\"\n",
    "kql_database = \"KQL_Demo_AIML_DB\"\n",
    "kql_table = \"KQL_pred\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kustoQuery = f\"{kql_table} | take 100\"\n",
    "kusto_df = spark.read\\\n",
    "                .format(\"com.microsoft.kusto.spark.synapse.datasource\")\\\n",
    "                .option(\"accessToken\", mssparkutils.credentials.getToken(kusto_uri))\\\n",
    "                .option(\"kustoCluster\", kusto_uri)\\\n",
    "                .option(\"kustoDatabase\", kql_database) \\\n",
    "                .option(\"kustoQuery\", kustoQuery).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(kusto_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Data from Lakehouse Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df = spark.read.format(\"delta\").load(\"Tables/predictive_maintenance_processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"records read: \" + str(input_df.count()))\n",
    "print(\"Schema: \")\n",
    "input_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_COL = 'failure'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "df_columns = input_df.columns\n",
    "df_columns.remove(TARGET_COL)\n",
    "\n",
    "# to make sure the TARGET_COL is the last column\n",
    "df = input_df.select(df_columns + [TARGET_COL]).withColumn(\n",
    "    TARGET_COL, F.col(TARGET_COL).cast(\"float\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Training and Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_pd = df.toPandas()\n",
    "df_pd = df_pd.astype(float)\n",
    "\n",
    "train, test = train_test_split(df_pd, test_size = 0.20)\n",
    "feature_cols = [c for c in df_pd.columns.tolist() if c not in [TARGET_COL]]\n",
    "\n",
    "df_pd.dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Track Machine Learning experiments and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "# Set given experiment as the active experiment. If an experiment with this name does not exist, a new experiment with this name is created.\n",
    "EXPERIMENT_NAME = \"ML_Predictive_Maintenance_Experiment\"\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "mlflow.autolog(disable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "smt = SMOTE()\n",
    "X_train_res, y_train_res = smt.fit_resample(train[feature_cols], train[TARGET_COL])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_res.value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forrest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow.sklearn\n",
    "import numpy as np\n",
    "from mlflow.models.signature import infer_signature\n",
    "from sklearn.metrics import f1_score, accuracy_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Start your training job with `start_run()`\n",
    "with mlflow.start_run() as run:\n",
    "    rfc_id = run.info.run_id\n",
    "    print(f\"run_id {rfc_id}, status: {run.info.status}\")\n",
    "    \n",
    "    rfc = RandomForestClassifier(max_depth = 5, n_estimators = 50)\n",
    "    rfc.fit(X_train_res, y_train_res)\n",
    "    signature = infer_signature(X_train_res, y_train_res)\n",
    "\n",
    "    mlflow.sklearn.log_model(\n",
    "        rfc,\n",
    "        \"predictive_maintenance_rfc\",\n",
    "        signature = signature,\n",
    "        registered_model_name = \"predictive_maintenance_rfc\"\n",
    "    ) \n",
    "\n",
    "    y_pred_train = rfc.predict(train[feature_cols])\n",
    "    # Calculate the classification metrics for test data\n",
    "    f1_train = f1_score(train[TARGET_COL], y_pred_train, average = 'weighted')\n",
    "    accuracy_train = accuracy_score(train[TARGET_COL], y_pred_train)\n",
    "    recall_train = recall_score(train[TARGET_COL], y_pred_train, average='weighted')\n",
    "\n",
    "    # Log the classification metrics to MLflow\n",
    "    mlflow.log_metric(\"f1_score_train\", f1_train)\n",
    "    mlflow.log_metric(\"accuracy_train\", accuracy_train)\n",
    "    mlflow.log_metric(\"recall_train\", recall_train)\n",
    "\n",
    "    # Print the run ID and the classification metrics\n",
    "    print(\"F1 score_train:\", f1_train)\n",
    "    print(\"Accuracy_train:\", accuracy_train)\n",
    "    print(\"Recall_train:\", recall_train)  \n",
    "\n",
    "    y_pred_test = rfc.predict(test[feature_cols])\n",
    "    # Calculate the classification metrics for test data\n",
    "    f1_test = f1_score(test[TARGET_COL], y_pred_test, average='weighted')\n",
    "    accuracy_test = accuracy_score(test[TARGET_COL], y_pred_test)\n",
    "    recall_test = recall_score(test[TARGET_COL], y_pred_test, average='weighted')\n",
    "\n",
    "    # Log the classification metrics to MLflow\n",
    "    mlflow.log_metric(\"f1_score_test\", f1_test)\n",
    "    mlflow.log_metric(\"accuracy_test\", accuracy_test)\n",
    "    mlflow.log_metric(\"recall_test\", recall_test)\n",
    "\n",
    "    # Print the classification metrics\n",
    "    print(\"F1 score_test:\", f1_test)\n",
    "    print(\"Accuracy_test:\", accuracy_test)\n",
    "    print(\"Recall_test:\", recall_test)\n",
    "\n",
    "    mlflow.log_param(\"model_name\", \"predictive_maintenance_rfc\")\n",
    "    print(\"All done\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Start your training job with `start_run()`\n",
    "with mlflow.start_run() as run:\n",
    "    lr_id = run.info.run_id\n",
    "    print(f\"run_id {lr_id}, status: {run.info.status}\")\n",
    "    \n",
    "    lr = LogisticRegression(random_state = 42)\n",
    "    lr.fit(X_train_res, y_train_res)\n",
    "    signature = infer_signature(X_train_res, y_train_res)\n",
    "\n",
    "    mlflow.sklearn.log_model(\n",
    "        lr,\n",
    "        \"predictive_maintenance_lr\",\n",
    "        signature = signature,\n",
    "        registered_model_name = \"predictive_maintenance_lr\"\n",
    "    ) \n",
    "\n",
    "    y_pred_train = lr.predict(train[feature_cols])\n",
    "    # Calculate the classification metrics for test data\n",
    "    f1_train = f1_score(train[TARGET_COL], y_pred_train, average = 'weighted')\n",
    "    accuracy_train = accuracy_score(train[TARGET_COL], y_pred_train)\n",
    "    recall_train = recall_score(train[TARGET_COL], y_pred_train, average='weighted')\n",
    "\n",
    "    # Log the classification metrics to MLflow\n",
    "    mlflow.log_metric(\"f1_score_train\", f1_train)\n",
    "    mlflow.log_metric(\"accuracy_train\", accuracy_train)\n",
    "    mlflow.log_metric(\"recall_train\", recall_train)\n",
    "\n",
    "    # Print the run ID and the classification metrics\n",
    "    print(\"F1 score_train:\", f1_train)\n",
    "    print(\"Accuracy_train:\", accuracy_train)\n",
    "    print(\"Recall_train:\", recall_train)  \n",
    "\n",
    "    y_pred_test = lr.predict(test[feature_cols])\n",
    "    # Calculate the classification metrics for test data\n",
    "    f1_test = f1_score(test[TARGET_COL], y_pred_test, average='weighted')\n",
    "    accuracy_test = accuracy_score(test[TARGET_COL], y_pred_test)\n",
    "    recall_test = recall_score(test[TARGET_COL], y_pred_test, average='weighted')\n",
    "\n",
    "    # Log the classification metrics to MLflow\n",
    "    mlflow.log_metric(\"f1_score_test\", f1_test)\n",
    "    mlflow.log_metric(\"accuracy_test\", accuracy_test)\n",
    "    mlflow.log_metric(\"recall_test\", recall_test)\n",
    "\n",
    "    # Print the classification metrics\n",
    "    print(\"F1 score_test:\", f1_test)\n",
    "    print(\"Accuracy_test:\", accuracy_test)\n",
    "    print(\"Recall_test:\", recall_test)\n",
    "\n",
    "    mlflow.log_param(\"model_name\", \"predictive_maintenance_lr\")\n",
    "    print(\"All done\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Start your training job with `start_run()`\n",
    "with mlflow.start_run() as run:\n",
    "    xgb_id = run.info.run_id\n",
    "    print(f\"run_id {xgb_id}, status: {run.info.status}\")\n",
    "    \n",
    "    xgb = XGBClassifier()\n",
    "    xgb.fit(X_train_res, y_train_res)\n",
    "    signature = infer_signature(X_train_res, y_train_res)\n",
    "\n",
    "    mlflow.xgboost.log_model(\n",
    "        xgb,\n",
    "        \"predictive_maintenance_xgb\",\n",
    "        signature = signature,\n",
    "        registered_model_name = \"predictive_maintenance_xgb\"\n",
    "    )\n",
    "\n",
    "    y_pred_train = xgb.predict(train[feature_cols])\n",
    "    # Calculate the classification metrics for test data\n",
    "    f1_train = f1_score(train[TARGET_COL], y_pred_train, average = 'weighted')\n",
    "    accuracy_train = accuracy_score(train[TARGET_COL], y_pred_train)\n",
    "    recall_train = recall_score(train[TARGET_COL], y_pred_train, average='weighted')\n",
    "\n",
    "    # Log the classification metrics to MLflow\n",
    "    mlflow.log_metric(\"f1_score_train\", f1_train)\n",
    "    mlflow.log_metric(\"accuracy_train\", accuracy_train)\n",
    "    mlflow.log_metric(\"recall_train\", recall_train)\n",
    "\n",
    "    # Print the run ID and the classification metrics\n",
    "    print(\"F1 score_train:\", f1_train)\n",
    "    print(\"Accuracy_train:\", accuracy_train)\n",
    "    print(\"Recall_train:\", recall_train)  \n",
    "\n",
    "    y_pred_test = xgb.predict(test[feature_cols])\n",
    "    # Calculate the classification metrics for test data\n",
    "    f1_test = f1_score(test[TARGET_COL], y_pred_test, average='weighted')\n",
    "    accuracy_test = accuracy_score(test[TARGET_COL], y_pred_test)\n",
    "    recall_test = recall_score(test[TARGET_COL], y_pred_test, average='weighted')\n",
    "\n",
    "    # Log the classification metrics to MLflow\n",
    "    mlflow.log_metric(\"f1_score_test\", f1_test)\n",
    "    mlflow.log_metric(\"accuracy_test\", accuracy_test)\n",
    "    mlflow.log_metric(\"recall_test\", recall_test)\n",
    "\n",
    "    # Print the classification metrics\n",
    "    print(\"F1 score_test:\", f1_test)\n",
    "    print(\"Accuracy_test:\", accuracy_test)\n",
    "    print(\"Recall_test:\", recall_test)\n",
    "\n",
    "    mlflow.log_param(\"model_name\", \"predictive_maintenance_xgb\")\n",
    "    print(\"All done\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlflow.search_runs(order_by = ['metrics.accuracy_test DESC'])\n",
    "ml_runs = mlflow.search_runs().drop(['metrics.score', 'params.alpha'], axis = 1).dropna()\n",
    "ml_runs = ml_runs[ml_runs[\"status\"] == \"FINISHED\"]\n",
    "ml_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run = ml_runs.sort_values(by = 'metrics.accuracy_train', ascending = False).iloc[0]\n",
    "best_run['run_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the Best Run\n",
    "new_model_version = mlflow.pyfunc.load_model(f\"runs:/{best_run['run_id']}/{best_run['params.model_name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model_version.predict(test[feature_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_model_version.metadata)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "746db0bae461d52c25b260aeab0d95c56770e76d1516a3abebd41cfaac0d13e8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
